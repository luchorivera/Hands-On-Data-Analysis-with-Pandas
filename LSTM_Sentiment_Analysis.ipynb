{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "LSTM Sentiment Analysis.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luchorivera/Hands-On-Data-Analysis-with-Pandas/blob/master/LSTM_Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObPyBKWz1Hym",
        "colab_type": "text"
      },
      "source": [
        "https://subscription.packtpub.com/book/data/9781789802740/8/ch08lvl1sec37/building-a-sentiment-analyzer-using-lstms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXG9OxM_tWS5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from string import punctuation\n",
        "import numpy as np\n",
        "import torch\n",
        "from nltk.tokenize import word_tokenize\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import json"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CSSnyeRtWS9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(\"sentiment labelled sentences/sentiment.txt\") as f:\n",
        "    reviews = f.read()\n",
        "    \n",
        "data = pd.DataFrame([review.split('\\t') for review in reviews.split('\\n')])"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySFngo0At1C2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cbe8832c-e7c2-44f8-97e9-2564045c44a8"
      },
      "source": [
        "data.shape"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3000, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdGNaVd8tuKI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "cd9e49e0-567a-4c37-9440-dd430ae95970"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A very, very, very slow-moving, aimless movie ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Not sure who was more lost - the flat characte...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Attempting artiness with black &amp; white and cle...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Very little music or anything to speak of.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The best scene in the movie was when Gerardo i...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   0  1\n",
              "0  A very, very, very slow-moving, aimless movie ...  0\n",
              "1  Not sure who was more lost - the flat characte...  0\n",
              "2  Attempting artiness with black & white and cle...  0\n",
              "3       Very little music or anything to speak of.    0\n",
              "4  The best scene in the movie was when Gerardo i...  1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9aoOdhYitsOe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data.columns = ['Review','Sentiment']\n",
        "\n",
        "data = data.sample(frac=1)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33D19UG6tWTA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "6cabcd21-7adc-43c3-cd27-32202a89ae20"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Review</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2078</th>\n",
              "      <td>I love this phone , It is very handy and has a...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1191</th>\n",
              "      <td>Both of the egg rolls were fantastic.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>684</th>\n",
              "      <td>This is definitely one of the better documenta...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2587</th>\n",
              "      <td>Also difficult to put on.I'd recommend avoidin...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>I don't think you will be disappointed.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 Review Sentiment\n",
              "2078  I love this phone , It is very handy and has a...         1\n",
              "1191              Both of the egg rolls were fantastic.         1\n",
              "684   This is definitely one of the better documenta...         1\n",
              "2587  Also difficult to put on.I'd recommend avoidin...         0\n",
              "100           I don't think you will be disappointed.           1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtk8iFE1uoF0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "ce36cfd2-f347-4ded-94b8-3acdf92da7bf"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8jazc7PtWTF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "9e566f11-8de0-48cf-9cfc-5619b6dac0a9"
      },
      "source": [
        "def split_words_reviews(data):\n",
        "    text = list(data['Review'].values)\n",
        "    clean_text = []\n",
        "    for t in text:\n",
        "        clean_text.append(t.translate(str.maketrans('', '', punctuation)).lower().rstrip())\n",
        "    tokenized = [word_tokenize(x) for x in clean_text]\n",
        "    all_text = []\n",
        "    for tokens in tokenized:\n",
        "        for t in tokens:\n",
        "            all_text.append(t)\n",
        "    return tokenized, set(all_text)\n",
        "\n",
        "reviews, vocab = split_words_reviews(data)\n",
        "\n",
        "reviews[0]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i',\n",
              " 'love',\n",
              " 'this',\n",
              " 'phone',\n",
              " 'it',\n",
              " 'is',\n",
              " 'very',\n",
              " 'handy',\n",
              " 'and',\n",
              " 'has',\n",
              " 'a',\n",
              " 'lot',\n",
              " 'of',\n",
              " 'features']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "VhhZYK_6tWTJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "776fb01a-d468-4b2d-9833-8bbc8cbcfafa"
      },
      "source": [
        "def create_dictionaries(words):\n",
        "    word_to_int_dict = {w:i+1 for i, w in enumerate(words)}\n",
        "    int_to_word_dict = {i:w for w, i in word_to_int_dict.items()}\n",
        "    return word_to_int_dict, int_to_word_dict\n",
        "\n",
        "word_to_int_dict, int_to_word_dict = create_dictionaries(vocab)\n",
        "\n",
        "int_to_word_dict"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{1: 'tracfonewebsite',\n",
              " 2: 'unfortunately',\n",
              " 3: 'lg',\n",
              " 4: 'sergeant',\n",
              " 5: 'protection',\n",
              " 6: 'stays',\n",
              " 7: 'v3c',\n",
              " 8: 'we',\n",
              " 9: 'welcome',\n",
              " 10: 'rotating',\n",
              " 11: '700w',\n",
              " 12: 'voiceovers',\n",
              " 13: 'camp',\n",
              " 14: 'raspberry',\n",
              " 15: 'verizons',\n",
              " 16: 'warm',\n",
              " 17: 'equally',\n",
              " 18: 'seconds',\n",
              " 19: 'refused',\n",
              " 20: 'machine',\n",
              " 21: 'supernatural',\n",
              " 22: 'threepack',\n",
              " 23: 'r',\n",
              " 24: 'makers',\n",
              " 25: 'scripts',\n",
              " 26: 'underlines',\n",
              " 27: 'underlying',\n",
              " 28: 'topvery',\n",
              " 29: 'frog',\n",
              " 30: 'obliged',\n",
              " 31: 'microsoft',\n",
              " 32: 'team',\n",
              " 33: 'regular',\n",
              " 34: 'division',\n",
              " 35: 'bowl',\n",
              " 36: 'racial',\n",
              " 37: 'defective',\n",
              " 38: 'player',\n",
              " 39: 'charging',\n",
              " 40: 'belmondo',\n",
              " 41: 'moment',\n",
              " 42: 'slideshow',\n",
              " 43: 'disliked',\n",
              " 44: 'realistic',\n",
              " 45: 'mesquite',\n",
              " 46: 'hearts',\n",
              " 47: 'stood',\n",
              " 48: 'eyepleasing',\n",
              " 49: 'years',\n",
              " 50: 'innocence',\n",
              " 51: 'patient',\n",
              " 52: 'spent',\n",
              " 53: 'loud',\n",
              " 54: 'sharing',\n",
              " 55: 'selfsacrifice',\n",
              " 56: 'an',\n",
              " 57: 'smoother',\n",
              " 58: 'ought',\n",
              " 59: 'portions',\n",
              " 60: 'uninteresting',\n",
              " 61: 'lord',\n",
              " 62: 'dollar',\n",
              " 63: 'bright',\n",
              " 64: 'scrimm',\n",
              " 65: 'dreary',\n",
              " 66: 'roasted',\n",
              " 67: 'given',\n",
              " 68: 'dvd',\n",
              " 69: 'raver',\n",
              " 70: 'romanticcharminghilariousand',\n",
              " 71: 'falwell',\n",
              " 72: 'fulfills',\n",
              " 73: 'ms',\n",
              " 74: '7',\n",
              " 75: 'firehouse',\n",
              " 76: 'sisters',\n",
              " 77: 'sony',\n",
              " 78: 'shirley',\n",
              " 79: 'topic',\n",
              " 80: 'song',\n",
              " 81: 'occupied',\n",
              " 82: 'attached',\n",
              " 83: 'backed',\n",
              " 84: 'succeeds',\n",
              " 85: 'compromise',\n",
              " 86: 'hawaiian',\n",
              " 87: 'reset',\n",
              " 88: 'holes',\n",
              " 89: 'repeated',\n",
              " 90: 'lock',\n",
              " 91: 'bully',\n",
              " 92: 'sprouts',\n",
              " 93: 'mystifying',\n",
              " 94: 'saving',\n",
              " 95: 'flavourful',\n",
              " 96: 'system',\n",
              " 97: 'tom',\n",
              " 98: 'autoanswer',\n",
              " 99: 'going',\n",
              " 100: 'chalkboard',\n",
              " 101: 'tuneful',\n",
              " 102: 'spot',\n",
              " 103: 'unfaithful',\n",
              " 104: 'exceptionally',\n",
              " 105: 'exchange',\n",
              " 106: 'rumbles',\n",
              " 107: 'immediately',\n",
              " 108: 'somewhere',\n",
              " 109: '10',\n",
              " 110: 'designed',\n",
              " 111: 'tedium',\n",
              " 112: 'empty',\n",
              " 113: 'usage',\n",
              " 114: 'officially',\n",
              " 115: 'church',\n",
              " 116: 'stagy',\n",
              " 117: 'intensity',\n",
              " 118: 'rests',\n",
              " 119: 'sensibility',\n",
              " 120: 'eye',\n",
              " 121: 'location',\n",
              " 122: 'tried',\n",
              " 123: 'eargels',\n",
              " 124: 'properly',\n",
              " 125: 'fond',\n",
              " 126: 'crowds',\n",
              " 127: 'despite',\n",
              " 128: 'details',\n",
              " 129: 'subject',\n",
              " 130: 'drama',\n",
              " 131: 'toasted',\n",
              " 132: 'lastly',\n",
              " 133: 'early',\n",
              " 134: 'passed',\n",
              " 135: 'dreams',\n",
              " 136: 'closeup',\n",
              " 137: 'caused',\n",
              " 138: 'imitation',\n",
              " 139: 'profiterole',\n",
              " 140: 'allowing',\n",
              " 141: 'camera',\n",
              " 142: 'hand',\n",
              " 143: 'mine',\n",
              " 144: 'tongue',\n",
              " 145: 'listener',\n",
              " 146: 'recieve',\n",
              " 147: 'relleno',\n",
              " 148: 'camelback',\n",
              " 149: 'who',\n",
              " 150: 'plethora',\n",
              " 151: 'vodka',\n",
              " 152: 'wings',\n",
              " 153: 'witnessed',\n",
              " 154: 'worked',\n",
              " 155: 'lights',\n",
              " 156: 'mother',\n",
              " 157: 'wed',\n",
              " 158: 'opportunity',\n",
              " 159: 'anyways',\n",
              " 160: 'paying',\n",
              " 161: 'reception',\n",
              " 162: 'vx',\n",
              " 163: 'amazingrge',\n",
              " 164: 'listed',\n",
              " 165: 'dumb',\n",
              " 166: 'true',\n",
              " 167: 'fun',\n",
              " 168: 'stranger',\n",
              " 169: 'speaking',\n",
              " 170: 'oh',\n",
              " 171: 'cod',\n",
              " 172: 'elk',\n",
              " 173: 'unpredictability',\n",
              " 174: 'asked',\n",
              " 175: 'greatness',\n",
              " 176: 'impressed',\n",
              " 177: 'dead',\n",
              " 178: 'crew',\n",
              " 179: 'slim',\n",
              " 180: 'drinking',\n",
              " 181: 'despicable',\n",
              " 182: 'refreshing',\n",
              " 183: 'extraordinary',\n",
              " 184: '17',\n",
              " 185: 'infatuated',\n",
              " 186: 'portraying',\n",
              " 187: 'driving',\n",
              " 188: 'every',\n",
              " 189: 'me',\n",
              " 190: 'finest',\n",
              " 191: 'customize',\n",
              " 192: 'steaks',\n",
              " 193: 'transfers',\n",
              " 194: 'george',\n",
              " 195: 'chickenwith',\n",
              " 196: 'speakerphone',\n",
              " 197: 'tony',\n",
              " 198: 'disgusted',\n",
              " 199: 'removing',\n",
              " 200: 'law',\n",
              " 201: 'balanced',\n",
              " 202: 'lugosi',\n",
              " 203: 'cartel',\n",
              " 204: 'problemvery',\n",
              " 205: 'koteasjack',\n",
              " 206: 'good4',\n",
              " 207: 'extra',\n",
              " 208: 'notice',\n",
              " 209: 'washed',\n",
              " 210: 'destroy',\n",
              " 211: 'meh',\n",
              " 212: 'entertainment',\n",
              " 213: 'borrowed',\n",
              " 214: 'spoiled',\n",
              " 215: 'apology',\n",
              " 216: 'players',\n",
              " 217: 'slowly',\n",
              " 218: 'greatno',\n",
              " 219: 'cheated',\n",
              " 220: 'worst',\n",
              " 221: 'meals',\n",
              " 222: 'tools',\n",
              " 223: 'max',\n",
              " 224: 'stupid',\n",
              " 225: 'g',\n",
              " 226: 'social',\n",
              " 227: 'monolog',\n",
              " 228: 'displeased',\n",
              " 229: 'nay',\n",
              " 230: 'literally',\n",
              " 231: 'street',\n",
              " 232: 'noca',\n",
              " 233: 'scares',\n",
              " 234: 'dosent',\n",
              " 235: 'to',\n",
              " 236: 'latterday',\n",
              " 237: 'hey',\n",
              " 238: 'frequentyly',\n",
              " 239: 'disapointing',\n",
              " 240: 'sent',\n",
              " 241: 'credits',\n",
              " 242: 'brunch',\n",
              " 243: 'cruel',\n",
              " 244: 'motivations',\n",
              " 245: 'stayed',\n",
              " 246: 'itbuy',\n",
              " 247: 'unbelievable',\n",
              " 248: 'seasonal',\n",
              " 249: 'ironically',\n",
              " 250: 'china',\n",
              " 251: 'specials',\n",
              " 252: 'mandalay',\n",
              " 253: 'anguish',\n",
              " 254: 'instant',\n",
              " 255: 'spoil',\n",
              " 256: 'ride',\n",
              " 257: 'lance',\n",
              " 258: 'receipt',\n",
              " 259: 'behold',\n",
              " 260: 'satisfied',\n",
              " 261: 'reservation',\n",
              " 262: 'mirrormask',\n",
              " 263: 'distressed',\n",
              " 264: 'dogs',\n",
              " 265: 'bus',\n",
              " 266: 'feisty',\n",
              " 267: 'flag',\n",
              " 268: 'speaker',\n",
              " 269: 'artiness',\n",
              " 270: 'strength',\n",
              " 271: 'bougth',\n",
              " 272: 'estevezs',\n",
              " 273: 'hits',\n",
              " 274: 'wire',\n",
              " 275: 'park',\n",
              " 276: 'waste',\n",
              " 277: 'seasoning',\n",
              " 278: 'drain',\n",
              " 279: 'workingeating',\n",
              " 280: 'work',\n",
              " 281: 'went',\n",
              " 282: 'angela',\n",
              " 283: 'season',\n",
              " 284: 'melted',\n",
              " 285: 'primary',\n",
              " 286: 'these',\n",
              " 287: 'southern',\n",
              " 288: 'universe',\n",
              " 289: 'port',\n",
              " 290: 'gripping',\n",
              " 291: 'want',\n",
              " 292: 'outta',\n",
              " 293: 'intoning',\n",
              " 294: 'adapter',\n",
              " 295: 'messes',\n",
              " 296: 'shipped',\n",
              " 297: 'flops',\n",
              " 298: 'patio',\n",
              " 299: 'continue',\n",
              " 300: 'correct',\n",
              " 301: 'corn',\n",
              " 302: 'face',\n",
              " 303: 'lifemy',\n",
              " 304: 'popcorn',\n",
              " 305: 'crispy',\n",
              " 306: 'most',\n",
              " 307: 'humans',\n",
              " 308: 'thorsen',\n",
              " 309: 'outstanding',\n",
              " 310: 'hear',\n",
              " 311: 'leftover',\n",
              " 312: 'lightweight',\n",
              " 313: 'fridays',\n",
              " 314: 'disaster',\n",
              " 315: 'smelled',\n",
              " 316: 'shows',\n",
              " 317: 'directed',\n",
              " 318: 'past',\n",
              " 319: 'myself',\n",
              " 320: 'overacting',\n",
              " 321: 'watered',\n",
              " 322: 'factbased',\n",
              " 323: 'sewer',\n",
              " 324: 'steep',\n",
              " 325: 'nine',\n",
              " 326: 'ebola',\n",
              " 327: 'ians',\n",
              " 328: 'until',\n",
              " 329: 'appointments',\n",
              " 330: 'cafe',\n",
              " 331: 'tacos',\n",
              " 332: 'sounded',\n",
              " 333: 'proven',\n",
              " 334: 'dozen',\n",
              " 335: 'cotton',\n",
              " 336: 'owneryou',\n",
              " 337: 'setup',\n",
              " 338: 'colorful',\n",
              " 339: 'peas',\n",
              " 340: 'hip',\n",
              " 341: 'jabra',\n",
              " 342: 'somewhat',\n",
              " 343: 'color',\n",
              " 344: 'candle',\n",
              " 345: 'ingredients',\n",
              " 346: '785',\n",
              " 347: 'made',\n",
              " 348: 'identify',\n",
              " 349: 'love',\n",
              " 350: 'superintelligent',\n",
              " 351: 'oldfashioned',\n",
              " 352: 'q',\n",
              " 353: 'ue',\n",
              " 354: 'burned',\n",
              " 355: 'members',\n",
              " 356: 'imperial',\n",
              " 357: 'right',\n",
              " 358: 'truck',\n",
              " 359: 'strip',\n",
              " 360: 'homework',\n",
              " 361: 'laughs',\n",
              " 362: 'reversible',\n",
              " 363: 'aurvåg',\n",
              " 364: 'ipods',\n",
              " 365: 'unforgettable',\n",
              " 366: 'usefulness',\n",
              " 367: 'shes',\n",
              " 368: 'schoolers',\n",
              " 369: 'pace',\n",
              " 370: 'besides',\n",
              " 371: 'rubberpetroleum',\n",
              " 372: 'matter',\n",
              " 373: 'petrified',\n",
              " 374: 'purcashed',\n",
              " 375: 'returning',\n",
              " 376: 'degree',\n",
              " 377: 'photography',\n",
              " 378: 'complexity',\n",
              " 379: 'wanted',\n",
              " 380: 'quality',\n",
              " 381: 'pants',\n",
              " 382: 'apart',\n",
              " 383: 'hoursthe',\n",
              " 384: 'accolades',\n",
              " 385: 'clarity',\n",
              " 386: 'herewhat',\n",
              " 387: 'liked',\n",
              " 388: 'settings',\n",
              " 389: 'jennifer',\n",
              " 390: 'nan',\n",
              " 391: 'good7',\n",
              " 392: 'shatner',\n",
              " 393: 'delights',\n",
              " 394: 'turned',\n",
              " 395: 'durable',\n",
              " 396: 'falling',\n",
              " 397: 'hairsplitting',\n",
              " 398: 'bodes',\n",
              " 399: 'brutal',\n",
              " 400: 'learned',\n",
              " 401: 'blacks',\n",
              " 402: 'dropped',\n",
              " 403: 'logitech',\n",
              " 404: 'regardless',\n",
              " 405: 'comparablypriced',\n",
              " 406: 'idiotic',\n",
              " 407: 'mickeys',\n",
              " 408: 'younger',\n",
              " 409: 'talent',\n",
              " 410: 'maintains',\n",
              " 411: 'toactivate',\n",
              " 412: 'excerpts',\n",
              " 413: 'currently',\n",
              " 414: 'hardest',\n",
              " 415: 'estevez',\n",
              " 416: 'spiffy',\n",
              " 417: 'ericsson',\n",
              " 418: 'whenever',\n",
              " 419: 'starts',\n",
              " 420: 'crowdpleaserthis',\n",
              " 421: 'fillet',\n",
              " 422: 'competitors',\n",
              " 423: 'events',\n",
              " 424: 'acting',\n",
              " 425: 'ideal',\n",
              " 426: 'girlfriendboyfriend',\n",
              " 427: 'ravoli',\n",
              " 428: 'setting',\n",
              " 429: 'han',\n",
              " 430: 'expensive',\n",
              " 431: 'hummh',\n",
              " 432: 'checked',\n",
              " 433: 'delicioso',\n",
              " 434: 'stir',\n",
              " 435: 'scamp',\n",
              " 436: 'overwhelmed',\n",
              " 437: 'utterly',\n",
              " 438: 'olivia',\n",
              " 439: 'loose',\n",
              " 440: 'cheesiness',\n",
              " 441: 'answer',\n",
              " 442: 'pure',\n",
              " 443: 'emptiness',\n",
              " 444: 'rings',\n",
              " 445: 'cole',\n",
              " 446: 'horrendous',\n",
              " 447: 'created',\n",
              " 448: 'tacky',\n",
              " 449: 'sins',\n",
              " 450: 'mentioned',\n",
              " 451: 'question',\n",
              " 452: 'jet',\n",
              " 453: 'finger',\n",
              " 454: 'kabuki',\n",
              " 455: 'hit',\n",
              " 456: 'replaced',\n",
              " 457: 'awful',\n",
              " 458: 'appetizer',\n",
              " 459: 'gake',\n",
              " 460: 'told',\n",
              " 461: 'duris',\n",
              " 462: 'was',\n",
              " 463: 'dirty',\n",
              " 464: 'steak',\n",
              " 465: 'come',\n",
              " 466: 'spends',\n",
              " 467: 'satifying',\n",
              " 468: 'miniusb',\n",
              " 469: 'recommending',\n",
              " 470: 'friendly',\n",
              " 471: 'awarded',\n",
              " 472: 'wellbalanced',\n",
              " 473: 'model',\n",
              " 474: 'worth',\n",
              " 475: 'try',\n",
              " 476: 'intelligence',\n",
              " 477: 'emily',\n",
              " 478: 'ringtones',\n",
              " 479: 'lot',\n",
              " 480: 'thoroughly',\n",
              " 481: 'ahead',\n",
              " 482: 'at',\n",
              " 483: 'flat',\n",
              " 484: 'sake',\n",
              " 485: 'sites',\n",
              " 486: 'native',\n",
              " 487: 'endall',\n",
              " 488: 'unfunny',\n",
              " 489: 'touches',\n",
              " 490: 'unbelievably',\n",
              " 491: 'less',\n",
              " 492: 'how',\n",
              " 493: 'ganoush',\n",
              " 494: 'wordofmouth',\n",
              " 495: 'feels',\n",
              " 496: 'curry',\n",
              " 497: 'barking',\n",
              " 498: 'everything',\n",
              " 499: 'circumstances',\n",
              " 500: 'glass',\n",
              " 501: 'dollars',\n",
              " 502: 'reviewing',\n",
              " 503: 'whoa',\n",
              " 504: 'blandly',\n",
              " 505: 'helpful',\n",
              " 506: 'ill',\n",
              " 507: 'french',\n",
              " 508: 'rancheros',\n",
              " 509: 'chips',\n",
              " 510: 'boogeyman',\n",
              " 511: 'cakes',\n",
              " 512: 'stinks',\n",
              " 513: 'total',\n",
              " 514: 'meatloaf',\n",
              " 515: 'bennett',\n",
              " 516: 'bluetoothmotorola',\n",
              " 517: 'think',\n",
              " 518: 'exemplars',\n",
              " 519: 'further',\n",
              " 520: 'guilt',\n",
              " 521: 'fishnet',\n",
              " 522: 'abroad',\n",
              " 523: 'days',\n",
              " 524: 'put',\n",
              " 525: 'deliciously',\n",
              " 526: 'youd',\n",
              " 527: 'remaining',\n",
              " 528: 'nut',\n",
              " 529: 'yet',\n",
              " 530: '8pm',\n",
              " 531: 'game',\n",
              " 532: 'convey',\n",
              " 533: 'beautiful',\n",
              " 534: 'marrow',\n",
              " 535: 'judge',\n",
              " 536: 'fried',\n",
              " 537: 'cook',\n",
              " 538: 'energetic',\n",
              " 539: 'hands',\n",
              " 540: 'dialogue',\n",
              " 541: 'truth',\n",
              " 542: 'can',\n",
              " 543: 'dealing',\n",
              " 544: 'mall',\n",
              " 545: 'vandiver',\n",
              " 546: 'slide',\n",
              " 547: 'seat',\n",
              " 548: 'providing',\n",
              " 549: 'patent',\n",
              " 550: 'eaten',\n",
              " 551: 'cinematic',\n",
              " 552: 'witty',\n",
              " 553: 'mozzarella',\n",
              " 554: 'cellphones',\n",
              " 555: 'lucio',\n",
              " 556: 'maria',\n",
              " 557: 'barren',\n",
              " 558: 'took',\n",
              " 559: 'tooth',\n",
              " 560: 'sources',\n",
              " 561: 'fixes',\n",
              " 562: 'v115g',\n",
              " 563: 'filmmaking',\n",
              " 564: 'iffy',\n",
              " 565: 'anthony',\n",
              " 566: 'fear',\n",
              " 567: 'prefer',\n",
              " 568: 'sizes',\n",
              " 569: 'armband',\n",
              " 570: 'ngage',\n",
              " 571: 'appalling',\n",
              " 572: 'such',\n",
              " 573: 'concrete',\n",
              " 574: 'duper',\n",
              " 575: 'specs',\n",
              " 576: 'awards',\n",
              " 577: 'half',\n",
              " 578: 'wifes',\n",
              " 579: 'wilkinson',\n",
              " 580: 'respect',\n",
              " 581: 'grey',\n",
              " 582: 'stari',\n",
              " 583: 'crocdodile',\n",
              " 584: 'release',\n",
              " 585: 'muppets',\n",
              " 586: 'crayonpencil',\n",
              " 587: 'verbal',\n",
              " 588: 'foods',\n",
              " 589: 'creaks',\n",
              " 590: 'spend',\n",
              " 591: 'touched',\n",
              " 592: 'relatively',\n",
              " 593: 'phrase',\n",
              " 594: 'cheerless',\n",
              " 595: 'black',\n",
              " 596: 'line',\n",
              " 597: 'hayworth',\n",
              " 598: 'gives',\n",
              " 599: 'cameo',\n",
              " 600: 'waitress',\n",
              " 601: 'fest',\n",
              " 602: 'foodand',\n",
              " 603: 'cannoli',\n",
              " 604: 'fav',\n",
              " 605: 'sashimi',\n",
              " 606: 'treasure',\n",
              " 607: 'alarm',\n",
              " 608: 'sean',\n",
              " 609: 'fabulous',\n",
              " 610: 'poorly',\n",
              " 611: 'pillow',\n",
              " 612: 'pastry',\n",
              " 613: 'smashburger',\n",
              " 614: 'genre',\n",
              " 615: '15',\n",
              " 616: 'cheeseburger',\n",
              " 617: 'its',\n",
              " 618: 'gluten',\n",
              " 619: 'endlessly',\n",
              " 620: 'twist',\n",
              " 621: 'masculinity',\n",
              " 622: 'goesthe',\n",
              " 623: 'gets',\n",
              " 624: 'definitly',\n",
              " 625: 'miyazakis',\n",
              " 626: 'trashy',\n",
              " 627: 'website',\n",
              " 628: 'amaze',\n",
              " 629: 'receptionsound',\n",
              " 630: 'delivers',\n",
              " 631: 'stress',\n",
              " 632: 'investment',\n",
              " 633: 'garbled',\n",
              " 634: 'brought',\n",
              " 635: 'eloquently',\n",
              " 636: 'need',\n",
              " 637: 'chains',\n",
              " 638: 'amazed',\n",
              " 639: 'blown',\n",
              " 640: 'storyline',\n",
              " 641: 'supposedly',\n",
              " 642: 'discarded',\n",
              " 643: 'sinister',\n",
              " 644: 'dialogs',\n",
              " 645: 'flawed',\n",
              " 646: 'liking',\n",
              " 647: 'senior',\n",
              " 648: 'larger',\n",
              " 649: 'having',\n",
              " 650: 'considered',\n",
              " 651: 'rescue',\n",
              " 652: 'sucked',\n",
              " 653: 'also',\n",
              " 654: 'disappointed',\n",
              " 655: 'pleased',\n",
              " 656: 'skimp',\n",
              " 657: 'hitch',\n",
              " 658: 'interpretations',\n",
              " 659: 'damage',\n",
              " 660: 'noteworthy',\n",
              " 661: 'waaaaaayyyyyyyyyy',\n",
              " 662: 'my',\n",
              " 663: 'easy',\n",
              " 664: 'holiday',\n",
              " 665: 'crêpe',\n",
              " 666: 'raw',\n",
              " 667: 'sketchy',\n",
              " 668: 'exceptional',\n",
              " 669: 'understand',\n",
              " 670: 'drains',\n",
              " 671: 'directly',\n",
              " 672: 'relate',\n",
              " 673: 'touch',\n",
              " 674: 'latest',\n",
              " 675: 'file',\n",
              " 676: 'arrived',\n",
              " 677: 'attempts',\n",
              " 678: 'martin',\n",
              " 679: 'lines',\n",
              " 680: 'howe',\n",
              " 681: 'tone',\n",
              " 682: 'roast',\n",
              " 683: 'noble',\n",
              " 684: 'moviemaking',\n",
              " 685: 'ready',\n",
              " 686: 'kirk',\n",
              " 687: 'scream',\n",
              " 688: 'sucks',\n",
              " 689: 'unwatchable',\n",
              " 690: 'suggestions',\n",
              " 691: 'exquisite',\n",
              " 692: 'bop',\n",
              " 693: 'array',\n",
              " 694: 'seating',\n",
              " 695: 'spoilers',\n",
              " 696: 'monica',\n",
              " 697: 'arrival',\n",
              " 698: 'static',\n",
              " 699: 'hoffmans',\n",
              " 700: 'cheesy',\n",
              " 701: 'courteous',\n",
              " 702: 'mouths',\n",
              " 703: 'brevity',\n",
              " 704: 'point',\n",
              " 705: 'documentaries',\n",
              " 706: 'offensive',\n",
              " 707: 'grim',\n",
              " 708: 'killing',\n",
              " 709: 'problems',\n",
              " 710: 'market',\n",
              " 711: 'hunan',\n",
              " 712: 'airline',\n",
              " 713: 'lyrics',\n",
              " 714: 'others',\n",
              " 715: 'thrillerhorror',\n",
              " 716: 'necklace',\n",
              " 717: 'admiration',\n",
              " 718: 'cooking',\n",
              " 719: 'script',\n",
              " 720: 'shame',\n",
              " 721: 'uninspired',\n",
              " 722: 'might',\n",
              " 723: 'screened',\n",
              " 724: 'merit',\n",
              " 725: 'cases',\n",
              " 726: '4s',\n",
              " 727: 'plan',\n",
              " 728: 'options',\n",
              " 729: 'running',\n",
              " 730: 'weve',\n",
              " 731: 'bland',\n",
              " 732: 'evil',\n",
              " 733: 'satisifed',\n",
              " 734: 'either',\n",
              " 735: 'bells',\n",
              " 736: 'ugly',\n",
              " 737: 'welldesigned',\n",
              " 738: 'bug',\n",
              " 739: 'motor',\n",
              " 740: 'omit',\n",
              " 741: 'wear',\n",
              " 742: 'pics',\n",
              " 743: 'parts',\n",
              " 744: 'airport',\n",
              " 745: 'specialand',\n",
              " 746: 'freeway',\n",
              " 747: 'frost',\n",
              " 748: 'whos',\n",
              " 749: 'temperaments',\n",
              " 750: 'instantly',\n",
              " 751: 'signal',\n",
              " 752: 'comfort',\n",
              " 753: 'operas',\n",
              " 754: 'brokeni',\n",
              " 755: 'schrader',\n",
              " 756: 'stage',\n",
              " 757: 'tons',\n",
              " 758: 'contract',\n",
              " 759: 'pros',\n",
              " 760: 'inches',\n",
              " 761: 'uplifting',\n",
              " 762: 'flatlined',\n",
              " 763: 'crisp',\n",
              " 764: 'turn',\n",
              " 765: 'giallo',\n",
              " 766: 'lion',\n",
              " 767: 'dozens',\n",
              " 768: 'curve',\n",
              " 769: 'humorous',\n",
              " 770: 'each',\n",
              " 771: 'evokes',\n",
              " 772: 'receives',\n",
              " 773: 'manufacturer',\n",
              " 774: 'questioning',\n",
              " 775: 'management',\n",
              " 776: 'sold',\n",
              " 777: 'infuriating',\n",
              " 778: 'melville',\n",
              " 779: 'bank',\n",
              " 780: 'fascinating',\n",
              " 781: 'without',\n",
              " 782: 'ninja',\n",
              " 783: 'figure',\n",
              " 784: 'occasions',\n",
              " 785: 'vomited',\n",
              " 786: 'attack',\n",
              " 787: 'valley',\n",
              " 788: 'seem',\n",
              " 789: 'favor',\n",
              " 790: 'cool',\n",
              " 791: 'borders',\n",
              " 792: 'college',\n",
              " 793: 'cutebut',\n",
              " 794: 'ability',\n",
              " 795: 'folks',\n",
              " 796: 'foodservice',\n",
              " 797: 'hiro',\n",
              " 798: 'blue',\n",
              " 799: 'difference',\n",
              " 800: 'ac',\n",
              " 801: 'complaint',\n",
              " 802: 'medium',\n",
              " 803: '12',\n",
              " 804: 'smudged',\n",
              " 805: 'garfield',\n",
              " 806: 'including',\n",
              " 807: 'soooooo',\n",
              " 808: 'seller',\n",
              " 809: 'designs',\n",
              " 810: 'explanation',\n",
              " 811: 'kits',\n",
              " 812: 'addition',\n",
              " 813: 'keyboard',\n",
              " 814: 'comprehensible',\n",
              " 815: 'kept',\n",
              " 816: 'dying',\n",
              " 817: 'drops',\n",
              " 818: 'app',\n",
              " 819: 'sydney',\n",
              " 820: 'sum',\n",
              " 821: 'cant',\n",
              " 822: 'crashed',\n",
              " 823: 'freaking',\n",
              " 824: 'detailed',\n",
              " 825: 'rocks',\n",
              " 826: 'goes',\n",
              " 827: 'angle',\n",
              " 828: 'watsons',\n",
              " 829: 'privileged',\n",
              " 830: 'twice',\n",
              " 831: 'evening',\n",
              " 832: 'connect',\n",
              " 833: 'characterisation',\n",
              " 834: 'provoking',\n",
              " 835: 'yummy',\n",
              " 836: 'flashbacks',\n",
              " 837: 'yelpers',\n",
              " 838: 'vocal',\n",
              " 839: 'ir',\n",
              " 840: 'nothingi',\n",
              " 841: 'delete',\n",
              " 842: 'kieslowski',\n",
              " 843: 'plate',\n",
              " 844: 'heres',\n",
              " 845: 'underappreciated',\n",
              " 846: 'sappiest',\n",
              " 847: 'sleek',\n",
              " 848: 'gold',\n",
              " 849: 'generates',\n",
              " 850: 'crawl',\n",
              " 851: 'hereas',\n",
              " 852: 'pocket',\n",
              " 853: 'realworld',\n",
              " 854: 'service',\n",
              " 855: 'make',\n",
              " 856: 'caught',\n",
              " 857: 'march',\n",
              " 858: 'pearls',\n",
              " 859: 'robert',\n",
              " 860: 'tolerate',\n",
              " 861: 'judo',\n",
              " 862: 'dit',\n",
              " 863: 'telly',\n",
              " 864: 'screamy',\n",
              " 865: 'counterfeit',\n",
              " 866: 'prelude',\n",
              " 867: 'since',\n",
              " 868: 'refurb',\n",
              " 869: 'narration',\n",
              " 870: 'real',\n",
              " 871: 'periodically',\n",
              " 872: 'flavorful',\n",
              " 873: 'schr450',\n",
              " 874: 'function',\n",
              " 875: 'thirty',\n",
              " 876: 'nicer',\n",
              " 877: 'childrens',\n",
              " 878: 'wellit',\n",
              " 879: 'unbearable',\n",
              " 880: 'changes',\n",
              " 881: 'frances',\n",
              " 882: 'disagree',\n",
              " 883: 'seemed',\n",
              " 884: 'darren',\n",
              " 885: 'ryans',\n",
              " 886: 'keypads',\n",
              " 887: 'rapidly',\n",
              " 888: 'trouble',\n",
              " 889: 'science',\n",
              " 890: 'kill',\n",
              " 891: 'pleasant',\n",
              " 892: 'succeeded',\n",
              " 893: 'aailiyah',\n",
              " 894: 'course',\n",
              " 895: 'smile',\n",
              " 896: 'bumpers',\n",
              " 897: 'protector',\n",
              " 898: 'mayo',\n",
              " 899: 'worlds',\n",
              " 900: 'selfindulgent',\n",
              " 901: 'oscar',\n",
              " 902: 'mexican',\n",
              " 903: 'content',\n",
              " 904: 'devine',\n",
              " 905: 'signs',\n",
              " 906: 'idealogical',\n",
              " 907: 'own',\n",
              " 908: 'jobs',\n",
              " 909: 'excelent',\n",
              " 910: 'joins',\n",
              " 911: 'avoiding',\n",
              " 912: 'orders',\n",
              " 913: 'kidnapped',\n",
              " 914: 'crocs',\n",
              " 915: 'interplay',\n",
              " 916: 'suffered',\n",
              " 917: 'redeeming',\n",
              " 918: 'youll',\n",
              " 919: 'sides',\n",
              " 920: 'tickets',\n",
              " 921: 'villains',\n",
              " 922: 'leeand',\n",
              " 923: 'correctly',\n",
              " 924: 'survivors',\n",
              " 925: 'eat',\n",
              " 926: 'five',\n",
              " 927: 'task',\n",
              " 928: 'allison',\n",
              " 929: 'dance',\n",
              " 930: 'restaraunt',\n",
              " 931: 'sea',\n",
              " 932: 'prays',\n",
              " 933: 'performance',\n",
              " 934: 'sending',\n",
              " 935: 'lunch',\n",
              " 936: 'margaritas',\n",
              " 937: 'menacing',\n",
              " 938: 'baba',\n",
              " 939: 'handmade',\n",
              " 940: 'completed',\n",
              " 941: 'dualpurpose',\n",
              " 942: 'played',\n",
              " 943: 'pizza',\n",
              " 944: 'while',\n",
              " 945: 'hopefully',\n",
              " 946: 'outdoor',\n",
              " 947: 'kitchen',\n",
              " 948: 'interested',\n",
              " 949: 'punches',\n",
              " 950: 'failed',\n",
              " 951: 'do',\n",
              " 952: 'europe',\n",
              " 953: 'cbr',\n",
              " 954: 'personable',\n",
              " 955: 'slid',\n",
              " 956: 'swung',\n",
              " 957: 'interface',\n",
              " 958: 'murky',\n",
              " 959: 'silently',\n",
              " 960: 'onethis',\n",
              " 961: 'positive',\n",
              " 962: 'containers',\n",
              " 963: 'sells',\n",
              " 964: 'poor',\n",
              " 965: 'may',\n",
              " 966: 'estate',\n",
              " 967: 'kanalys',\n",
              " 968: 'florida',\n",
              " 969: 'warranty',\n",
              " 970: 'hot',\n",
              " 971: 'poignant',\n",
              " 972: 'moral',\n",
              " 973: 'academy',\n",
              " 974: 'underbite',\n",
              " 975: 'ratings',\n",
              " 976: 'off',\n",
              " 977: 'bread',\n",
              " 978: 'waitresses',\n",
              " 979: 'possible',\n",
              " 980: 'bouchon',\n",
              " 981: 'min',\n",
              " 982: 'powerful',\n",
              " 983: 'flash',\n",
              " 984: 'swamp',\n",
              " 985: 'cable',\n",
              " 986: 'thought',\n",
              " 987: 'mushrooms',\n",
              " 988: 'sex',\n",
              " 989: 'cancan',\n",
              " 990: 'junk',\n",
              " 991: 'grasp',\n",
              " 992: 'his',\n",
              " 993: 'sangria',\n",
              " 994: 'forced',\n",
              " 995: 'ebay',\n",
              " 996: 'akin',\n",
              " 997: '1973',\n",
              " 998: 'deadpan',\n",
              " 999: 'guards',\n",
              " 1000: 'insulin',\n",
              " ...}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwlssqPctWTN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('word_to_int_dict.json', 'w') as fp:\n",
        "    json.dump(word_to_int_dict, fp)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knUqUs9ltWTR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "9de661ee-6d26-44e0-a86e-339e18267334"
      },
      "source": [
        "print(np.max([len(x) for x in reviews]))\n",
        "print(np.mean([len(x) for x in reviews]))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "70\n",
            "11.783666666666667\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XEGwD83otWTX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "034d34c6-d02d-4747-f423-aebc99992587"
      },
      "source": [
        "def pad_text(tokenized_reviews, seq_length):\n",
        "    \n",
        "    reviews = []\n",
        "    \n",
        "    for review in tokenized_reviews:\n",
        "        if len(review) >= seq_length:\n",
        "            reviews.append(review[:seq_length])\n",
        "        else:\n",
        "            reviews.append(['']*(seq_length-len(review)) + review)\n",
        "        \n",
        "    return np.array(reviews)\n",
        "\n",
        "padded_sentences = pad_text(reviews, seq_length = 50)\n",
        "\n",
        "padded_sentences[0]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '',\n",
              "       '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '',\n",
              "       '', '', 'i', 'love', 'this', 'phone', 'it', 'is', 'very', 'handy',\n",
              "       'and', 'has', 'a', 'lot', 'of', 'features'], dtype='<U33')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5blU8VLtWTb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "int_to_word_dict[0] = ''\n",
        "word_to_int_dict[''] = 0"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkW0rhWQtWTd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "1422f30f-3ee6-4cf3-ca19-d49e156928aa"
      },
      "source": [
        "encoded_sentences = np.array([[word_to_int_dict[word] for word in review] for review in padded_sentences])\n",
        "\n",
        "encoded_sentences[0]"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0, 4866,  349, 1573, 2577, 3413, 4459, 1340, 3692,\n",
              "       4829, 4481, 2386,  479, 1499, 1154])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4MiNiLaOtWTg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SentimentLSTM(nn.Module):\n",
        "    \n",
        "    def __init__(self, n_vocab, n_embed, n_hidden, n_output, n_layers, drop_p = 0.8):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.n_vocab = n_vocab  \n",
        "        self.n_layers = n_layers \n",
        "        self.n_hidden = n_hidden \n",
        "        \n",
        "        self.embedding = nn.Embedding(n_vocab, n_embed)\n",
        "        self.lstm = nn.LSTM(n_embed, n_hidden, n_layers, batch_first = True, dropout = drop_p)\n",
        "        self.dropout = nn.Dropout(drop_p)\n",
        "        self.fc = nn.Linear(n_hidden, n_output)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        \n",
        "        \n",
        "    def forward (self, input_words):\n",
        "                          \n",
        "        embedded_words = self.embedding(input_words)\n",
        "        lstm_out, h = self.lstm(embedded_words) \n",
        "        lstm_out = self.dropout(lstm_out)\n",
        "        lstm_out = lstm_out.contiguous().view(-1, self.n_hidden)\n",
        "        fc_out = self.fc(lstm_out)                  \n",
        "        sigmoid_out = self.sigmoid(fc_out)              \n",
        "        sigmoid_out = sigmoid_out.view(batch_size, -1)  \n",
        "        \n",
        "        sigmoid_last = sigmoid_out[:, -1]\n",
        "        \n",
        "        return sigmoid_last, h\n",
        "    \n",
        "    \n",
        "    def init_hidden (self, batch_size):\n",
        "        \n",
        "        device = \"cpu\"\n",
        "        weights = next(self.parameters()).data\n",
        "        h = (weights.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device),\n",
        "             weights.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device))\n",
        "        \n",
        "        return h\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Z3Zr3BvtWTj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_vocab = len(word_to_int_dict)\n",
        "n_embed = 50\n",
        "n_hidden = 100\n",
        "n_output = 1\n",
        "n_layers = 2\n",
        "\n",
        "net = SentimentLSTM(n_vocab, n_embed, n_hidden, n_output, n_layers)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1R3cHktdtWTn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels = np.array([int(x) for x in data['Sentiment'].values])\n",
        "\n",
        "train_ratio = 0.8\n",
        "valid_ratio = (1 - train_ratio)/2\n",
        "\n",
        "total = len(encoded_sentences)\n",
        "train_cutoff = int(total * train_ratio)\n",
        "valid_cutoff = int(total * (1 - valid_ratio))\n",
        "\n",
        "train_x, train_y = torch.Tensor(encoded_sentences[:train_cutoff]).long(), torch.Tensor(labels[:train_cutoff]).long()\n",
        "valid_x, valid_y = torch.Tensor(encoded_sentences[train_cutoff : valid_cutoff]).long(), torch.Tensor(labels[train_cutoff : valid_cutoff]).long()\n",
        "test_x, test_y = torch.Tensor(encoded_sentences[valid_cutoff:]).long(), torch.Tensor(labels[valid_cutoff:])\n",
        "\n",
        "train_data = TensorDataset(train_x, train_y)\n",
        "valid_data = TensorDataset(valid_x, valid_y)\n",
        "test_data = TensorDataset(test_x, test_y)\n",
        "\n",
        "batch_size = 1\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size = batch_size, shuffle = True)\n",
        "valid_loader = DataLoader(valid_data, batch_size = batch_size, shuffle = True)\n",
        "test_loader = DataLoader(test_data, batch_size = batch_size, shuffle = True)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FC2SeEyctWTq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print_every = 2400\n",
        "step = 0\n",
        "n_epochs = 3\n",
        "clip = 5  \n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr = 0.001)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7julLxQtWTt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "e2dc1b77-f3c5-4c8d-cca5-20dd2d5f9c13"
      },
      "source": [
        "for epoch in range(n_epochs):\n",
        "    h = net.init_hidden(batch_size)\n",
        "    \n",
        "    for inputs, labels in train_loader:\n",
        "        step += 1  \n",
        "        net.zero_grad()\n",
        "        output, h = net(inputs)\n",
        "        loss = criterion(output.squeeze(), labels.float())\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm(net.parameters(), clip)\n",
        "        optimizer.step()\n",
        "        \n",
        "        if (step % print_every) == 0:            \n",
        "            net.eval()\n",
        "            valid_losses = []\n",
        "\n",
        "            for v_inputs, v_labels in valid_loader:\n",
        "                       \n",
        "                v_output, v_h = net(v_inputs)\n",
        "                v_loss = criterion(v_output.squeeze(), v_labels.float())\n",
        "                valid_losses.append(v_loss.item())\n",
        "\n",
        "            print(\"Epoch: {}/{}\".format((epoch+1), n_epochs),\n",
        "                  \"Step: {}\".format(step),\n",
        "                  \"Training Loss: {:.4f}\".format(loss.item()),\n",
        "                  \"Validation Loss: {:.4f}\".format(np.mean(valid_losses)))\n",
        "            net.train()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:529: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])) is deprecated. Please ensure they have the same size.\n",
            "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/3 Step: 2400 Training Loss: 0.9147 Validation Loss: 0.6288\n",
            "Epoch: 2/3 Step: 4800 Training Loss: 0.8034 Validation Loss: 0.5738\n",
            "Epoch: 3/3 Step: 7200 Training Loss: 0.1607 Validation Loss: 0.6199\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DVkcIDUtWTw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(net.state_dict(), 'model.pkl')"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41caJicxtWTz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5b8fc2cf-551b-4324-b749-f5f1927178b0"
      },
      "source": [
        "net = SentimentLSTM(n_vocab, n_embed, n_hidden, n_output, n_layers)\n",
        "net.load_state_dict(torch.load('model.pkl'))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-S25T2btWT3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "a66e1fc2-8b1e-4a28-d173-605422b81318"
      },
      "source": [
        "net.eval()\n",
        "test_losses = []\n",
        "num_correct = 0\n",
        "\n",
        "for inputs, labels in test_loader:\n",
        "\n",
        "    test_output, test_h = net(inputs)\n",
        "    loss = criterion(test_output, labels)\n",
        "    test_losses.append(loss.item())\n",
        "    \n",
        "    preds = torch.round(test_output.squeeze())\n",
        "    correct_tensor = preds.eq(labels.float().view_as(preds))\n",
        "    correct = np.squeeze(correct_tensor.numpy())\n",
        "    num_correct += np.sum(correct)\n",
        "    \n",
        "print(\"Test Loss: {:.4f}\".format(np.mean(test_losses)))\n",
        "print(\"Test Accuracy: {:.2f}\".format(num_correct/len(test_loader.dataset)))    "
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 0.5811\n",
            "Test Accuracy: 0.78\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0yWvsWotWT8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_review(review):\n",
        "    review = review.translate(str.maketrans('', '', punctuation)).lower().rstrip()\n",
        "    tokenized = word_tokenize(review)\n",
        "    if len(tokenized) >= 50:\n",
        "        review = tokenized[:50]\n",
        "    else:\n",
        "        review= ['0']*(50-len(tokenized)) + tokenized\n",
        "    \n",
        "    final = []\n",
        "    \n",
        "    for token in review:\n",
        "        try:\n",
        "            final.append(word_to_int_dict[token])\n",
        "            \n",
        "        except:\n",
        "            final.append(word_to_int_dict[''])\n",
        "        \n",
        "    return final"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nFcpQcatWUB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(review):\n",
        "    net.eval()\n",
        "    words = np.array([preprocess_review(review)])\n",
        "    padded_words = torch.from_numpy(words)\n",
        "    pred_loader = DataLoader(padded_words, batch_size = 1, shuffle = True)\n",
        "    for x in pred_loader:\n",
        "        output = net(x)[0].item()\n",
        "    \n",
        "    msg = \"This is a positive review.\" if output >= 0.5 else \"This is a negative review.\"\n",
        "    print(msg)\n",
        "    print('Prediction = ' + str(output))"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPp6aMlStWUE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "97fa934c-a6bf-46c6-b871-42a99c34d739"
      },
      "source": [
        "predict(\"The film was good\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This is a positive review.\n",
            "Prediction = 0.9511520266532898\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "loWRJSehtWUI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "a94279c4-e076-4db2-9f4c-210619f4e9c8"
      },
      "source": [
        "predict(\"It was not good\")"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This is a negative review.\n",
            "Prediction = 0.03561911731958389\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bL645WSa015-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}